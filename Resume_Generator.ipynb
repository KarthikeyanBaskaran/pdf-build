{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/KarthikeyanBaskaran/pdf-build/blob/main/Resume_Generator.ipynb",
      "authorship_tag": "ABX9TyPCaZx/Db0SJumCv0pb9W3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthikeyanBaskaran/pdf-build/blob/main/Resume_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how can i pip install without output steps\n",
        "\n",
        "!pip install --quiet groq\n"
      ],
      "metadata": {
        "id": "6rCgqRLsZqUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jPcFmp3B7QJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! apt-get install git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKt6eA3dz5yW",
        "outputId": "2d20d3f1-58ba-4640-f410-f8333cafec03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KarthikeyanBaskaran/pdf-build.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWKz0kvz8yY",
        "outputId": "7ebf8ab8-7238-4167-d521-a93bf4d4e225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pdf-build'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/39)\u001b[K\rremote: Counting objects:   5% (2/39)\u001b[K\rremote: Counting objects:   7% (3/39)\u001b[K\rremote: Counting objects:  10% (4/39)\u001b[K\rremote: Counting objects:  12% (5/39)\u001b[K\rremote: Counting objects:  15% (6/39)\u001b[K\rremote: Counting objects:  17% (7/39)\u001b[K\rremote: Counting objects:  20% (8/39)\u001b[K\rremote: Counting objects:  23% (9/39)\u001b[K\rremote: Counting objects:  25% (10/39)\u001b[K\rremote: Counting objects:  28% (11/39)\u001b[K\rremote: Counting objects:  30% (12/39)\u001b[K\rremote: Counting objects:  33% (13/39)\u001b[K\rremote: Counting objects:  35% (14/39)\u001b[K\rremote: Counting objects:  38% (15/39)\u001b[K\rremote: Counting objects:  41% (16/39)\u001b[K\rremote: Counting objects:  43% (17/39)\u001b[K\rremote: Counting objects:  46% (18/39)\u001b[K\rremote: Counting objects:  48% (19/39)\u001b[K\rremote: Counting objects:  51% (20/39)\u001b[K\rremote: Counting objects:  53% (21/39)\u001b[K\rremote: Counting objects:  56% (22/39)\u001b[K\rremote: Counting objects:  58% (23/39)\u001b[K\rremote: Counting objects:  61% (24/39)\u001b[K\rremote: Counting objects:  64% (25/39)\u001b[K\rremote: Counting objects:  66% (26/39)\u001b[K\rremote: Counting objects:  69% (27/39)\u001b[K\rremote: Counting objects:  71% (28/39)\u001b[K\rremote: Counting objects:  74% (29/39)\u001b[K\rremote: Counting objects:  76% (30/39)\u001b[K\rremote: Counting objects:  79% (31/39)\u001b[K\rremote: Counting objects:  82% (32/39)\u001b[K\rremote: Counting objects:  84% (33/39)\u001b[K\rremote: Counting objects:  87% (34/39)\u001b[K\rremote: Counting objects:  89% (35/39)\u001b[K\rremote: Counting objects:  92% (36/39)\u001b[K\rremote: Counting objects:  94% (37/39)\u001b[K\rremote: Counting objects:  97% (38/39)\u001b[K\rremote: Counting objects: 100% (39/39)\u001b[K\rremote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/36)\u001b[K\rremote: Compressing objects:   5% (2/36)\u001b[K\rremote: Compressing objects:   8% (3/36)\u001b[K\rremote: Compressing objects:  11% (4/36)\u001b[K\rremote: Compressing objects:  13% (5/36)\u001b[K\rremote: Compressing objects:  16% (6/36)\u001b[K\rremote: Compressing objects:  19% (7/36)\u001b[K\rremote: Compressing objects:  22% (8/36)\u001b[K\rremote: Compressing objects:  25% (9/36)\u001b[K\rremote: Compressing objects:  27% (10/36)\u001b[K\rremote: Compressing objects:  30% (11/36)\u001b[K\rremote: Compressing objects:  33% (12/36)\u001b[K\rremote: Compressing objects:  36% (13/36)\u001b[K\rremote: Compressing objects:  38% (14/36)\u001b[K\rremote: Compressing objects:  41% (15/36)\u001b[K\rremote: Compressing objects:  44% (16/36)\u001b[K\rremote: Compressing objects:  47% (17/36)\u001b[K\rremote: Compressing objects:  50% (18/36)\u001b[K\rremote: Compressing objects:  52% (19/36)\u001b[K\rremote: Compressing objects:  55% (20/36)\u001b[K\rremote: Compressing objects:  58% (21/36)\u001b[K\rremote: Compressing objects:  61% (22/36)\u001b[K\rremote: Compressing objects:  63% (23/36)\u001b[K\rremote: Compressing objects:  66% (24/36)\u001b[K\rremote: Compressing objects:  69% (25/36)\u001b[K\rremote: Compressing objects:  72% (26/36)\u001b[K\rremote: Compressing objects:  75% (27/36)\u001b[K\rremote: Compressing objects:  77% (28/36)\u001b[K\rremote: Compressing objects:  80% (29/36)\u001b[K\rremote: Compressing objects:  83% (30/36)\u001b[K\rremote: Compressing objects:  86% (31/36)\u001b[K\rremote: Compressing objects:  88% (32/36)\u001b[K\rremote: Compressing objects:  91% (33/36)\u001b[K\rremote: Compressing objects:  94% (34/36)\u001b[K\rremote: Compressing objects:  97% (35/36)\u001b[K\rremote: Compressing objects: 100% (36/36)\u001b[K\rremote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "Receiving objects:   2% (1/39)\rReceiving objects:   5% (2/39)\rReceiving objects:   7% (3/39)\rReceiving objects:  10% (4/39)\rReceiving objects:  12% (5/39)\rReceiving objects:  15% (6/39)\rReceiving objects:  17% (7/39)\rReceiving objects:  20% (8/39)\rReceiving objects:  23% (9/39)\rReceiving objects:  25% (10/39)\rReceiving objects:  28% (11/39)\rReceiving objects:  30% (12/39)\rReceiving objects:  33% (13/39)\rReceiving objects:  35% (14/39)\rReceiving objects:  38% (15/39)\rReceiving objects:  41% (16/39)\rReceiving objects:  43% (17/39)\rReceiving objects:  46% (18/39)\rremote: Total 39 (delta 19), reused 12 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  48% (19/39)\rReceiving objects:  51% (20/39)\rReceiving objects:  53% (21/39)\rReceiving objects:  56% (22/39)\rReceiving objects:  58% (23/39)\rReceiving objects:  61% (24/39)\rReceiving objects:  64% (25/39)\rReceiving objects:  66% (26/39)\rReceiving objects:  69% (27/39)\rReceiving objects:  71% (28/39)\rReceiving objects:  74% (29/39)\rReceiving objects:  76% (30/39)\rReceiving objects:  79% (31/39)\rReceiving objects:  82% (32/39)\rReceiving objects:  84% (33/39)\rReceiving objects:  87% (34/39)\rReceiving objects:  89% (35/39)\rReceiving objects:  92% (36/39)\rReceiving objects:  94% (37/39)\rReceiving objects:  97% (38/39)\rReceiving objects: 100% (39/39)\rReceiving objects: 100% (39/39), 107.29 KiB | 15.33 MiB/s, done.\n",
            "Resolving deltas:   0% (0/19)\rResolving deltas:   5% (1/19)\rResolving deltas:  10% (2/19)\rResolving deltas:  15% (3/19)\rResolving deltas:  21% (4/19)\rResolving deltas:  26% (5/19)\rResolving deltas:  31% (6/19)\rResolving deltas:  36% (7/19)\rResolving deltas:  42% (8/19)\rResolving deltas:  47% (9/19)\rResolving deltas:  52% (10/19)\rResolving deltas:  57% (11/19)\rResolving deltas:  63% (12/19)\rResolving deltas:  68% (13/19)\rResolving deltas:  73% (14/19)\rResolving deltas:  78% (15/19)\rResolving deltas:  84% (16/19)\rResolving deltas:  89% (17/19)\rResolving deltas:  94% (18/19)\rResolving deltas: 100% (19/19)\rResolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jd = \"\"\"\n",
        "StackAdapt is a self-serve advertising platform that specializes in multi-channel solutions including native, display, video, connected TV, audio, in-game, and digital out-of-home ads. We empower hundreds of digitally-focused companies to deliver outcomes and exceptional campaign performance everyday. StackAdapt was founded with a vision to be more than an advertising platform, it’s a hub of innovation, imagination and creativity.\n",
        "\n",
        "We're looking to add Data Engineers to our data science team! This team works on solving complex problems for StackAdapt's digital advertising platform. You'll be working directly with our data scientists, data engineers, Engineering team, and CTO on building pipelines and ad optimization models. With databases that process millions of requests per second, there's no shortage of data and problems to tackle.\n",
        "\n",
        "Want to learn more about our Data Science Team: https://alldus.com/ie/blog/podcasts/aiinaction-ned-dimitrov-stackadapt/\n",
        "\n",
        "Learn more about our team culture here: https://www.stackadapt.com/careers/data-science\n",
        "\n",
        "Watch our talk at Amazon Tech Talks: https://www.youtube.com/watch?v=lRqu-a4gPuU\n",
        "\n",
        "StackAdapt is a Remote First company, and we are open to candidates in various of our operating locations throughout the globe!\n",
        "\n",
        "What you'll be doing:\n",
        "\n",
        "Design modular and scalable real time data pipelines to handle huge datasets\n",
        "Understand and implement custom ML algorithms in a low latency environment\n",
        "Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently\n",
        "\n",
        "What you'll bring to the table:\n",
        "\n",
        "Have the ability to take an ambiguously defined task, and break it down into actionable steps\n",
        "Have deep understanding of algorithm and software design, concurrency, and data structures\n",
        "Experience in implementing probabilistic or machine learning algorithms\n",
        "Interest in designing scalable distributed systems\n",
        "A high GPA from a well-respected Computer Science program\n",
        "Enjoy working in a friendly, collaborative environment with others\n",
        "\n",
        "StackAdapters enjoy:\n",
        "\n",
        "Competitive salary + equity\n",
        "RRSP matching\n",
        "3 weeks vacation + 3 personal care days + 1 Culture & Belief day + birthdays off\n",
        "Access to a comprehensive mental health care platform\n",
        "Health benefits from day one of employment\n",
        "Work from home reimbursements\n",
        "Optional global WeWork membership for those who want a change from their home office\n",
        "Robust training and onboarding program\n",
        "Coverage and support of personal development initiatives (conferences, courses, etc)\n",
        "Access to StackAdapt programmatic courses and certifications to support continuous learning\n",
        "Mentorship opportunities with industry leaders\n",
        "An awesome parental leave policy\n",
        "A friendly, welcoming, and supportive culture\n",
        "Our social and team events!\n",
        "\n",
        "If this role speaks to you then please apply - we'd love to speak with you. Due to a high volume of interest, only those shortlisted for interview will be contacted.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FWmmCBGhQ3Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyResume = \"\"\"Professional Experience:\n",
        "1. Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023):\n",
        "•\tAchieved a 15% reduction in procurement costs by developing a data-driven supplier evaluation model and conducting detailed cost analysis.\n",
        "•\tImproved project timelines by 20% through predictive analytics for demand forecasting and proactive delay management.\n",
        "•\tSaved $85M annually by developing and implementing Python-based automation tools, streamlining procurement workflows and enhancing operational efficiency.\n",
        "•\tEnhanced supplier performance monitoring by designing vendor scorecards using Excel and Power BI, enabling data visualization for better decision-making.\n",
        "•\tIntegrated IT systems into procurement processes, deploying Microsoft Power Query and SharePoint to automate reporting and improve data accuracy, reducing manual efforts by 30%.\n",
        "•\tDeveloped a procurement dashboard in Power BI to track KPIs, supplier performance, and category spend, enabling leadership to make informed decisions.\n",
        "•\tOptimized material master data accuracy through automation, improving data consistency and reducing errors by 25%.\n",
        "•\tCollaborated with IT and cross-functional teams to develop scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems.\n",
        "•\tImplemented risk mitigation strategies by analyzing supply chain data and identifying alternative sourcing options to address potential delivery bottlenecks.\n",
        "•\tStreamlined RFQ processes using advanced quote comparison tools and automation, reducing processing time by 40%.\n",
        "•\tNegotiated supplier contracts to achieve cost savings and ensure compliance with government policies, leveraging IT tools for real-time data tracking.\n",
        "•\tDesigned and deployed customized procurement strategies to handle delivery complexities, ensuring continuity in production and minimizing downtime.\n",
        "•\tImproved KPI tracking by automating SLA adherence reporting, increasing visibility for stakeholders and reducing manual intervention.\n",
        "•\tLed IT-driven innovations in procurement by identifying opportunities for automation and implementing solutions that enhanced scalability and flexibility across procurement functions.\n",
        "\n",
        "2. Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021):\n",
        "•\tReduced production costs by 10% through process optimization analyses, utilizing data-driven insights to streamline operations.\n",
        "•\tIdentified potential savings of €3M by analyzing supplier costs and payment credit periods, implementing standardized and efficient procurement strategies.\n",
        "•\tImproved operational efficiency by 15% by leading cross-functional collaboration and deploying Lean methodologies.\n",
        "•\tEnhanced project delivery capacity by 20% through process improvements that freed up resources for handling additional projects.\n",
        "•\tDeveloped and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks.\n",
        "•\tLed RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs.\n",
        "•\tCollaborated with product management and design teams to define project scope and requirements, ensuring alignment with customer needs and delivering projects 10% faster.\n",
        "•\tBuilt Python-based automation scripts, optimizing internal workflows and reducing manual effort by 25%, enhancing team efficiency.\n",
        "•\tStrengthened stakeholder relationships by implementing efficient communication strategies, improving supplier and sponsor engagement by 12%.\n",
        "•\tDeveloped dashboards and reporting tools using Python and cloud services to monitor KPIs, improving decision-making accuracy by 18%.\n",
        "\n",
        "3. Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021):\n",
        "•\tManaged RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
        "•\tBuilt and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency.\n",
        "•\tExecuted supplier auctions using digital platforms, reducing overall part costs by 8-12% while ensuring competitive pricing.\n",
        "•\tPerformed zero-based costing using advanced Excel (macros, pivot tables) and analytical tools, enabling 15% cost savings through data-driven negotiations.\n",
        "•\tDeveloped Power BI dashboards to track procurement KPIs, cutting reporting time by 40% and enhancing supplier performance monitoring.\n",
        "•\tUtilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
        "•\tIntegrated ERP tools (SAP) to streamline procurement workflows, reducing manual effort by 30% and enhancing process accuracy.\n",
        "•\tSourced globally from cost-effective suppliers to meet production demands, ensuring 100% on-time delivery and adherence to quality standards.\n",
        "•\tGained expertise in manufacturing techniques, achieving a 10% reduction in project costs by aligning processes with best-cost practices.\n",
        "•\tCollaborated on ERP system enhancements, reducing procurement cycle times by 20% and improving data accessibility for stakeholders.\n",
        "\n",
        "\n",
        "Education:\n",
        " Diploma in Data Analytics for Business, St. Clair College (2024):\n",
        " GPA: 4.0\n",
        "    Relevant coursework: Data Mining, Predictive Analytics, Database Management, and Machine Learning.\n",
        "\n",
        " Bachelor of Engineering, Mechanical,  St. Joseph’s institute of Technology (2024):\n",
        "   GPA: 3.5\n",
        "\n",
        "Certifications:\n",
        " SQL for Data Science (Coursera)\n",
        " Python for Data Analysis (Udemy)\n",
        " Tableau Desktop Specialist Certification\n",
        "\n",
        "#### Projects:\n",
        "1. Meta Financial Dashboard\n",
        "•\tDeveloped an interactive Tableau dashboard to analyze key financial KPIs such as P/E Ratio, Current Ratio, and Debt to Equity.\n",
        "•\tDelivered actionable insights that improved financial decisionmaking, enhancing reporting accuracy and strategic planning.\n",
        "2. Customer Churn Analysis\n",
        "•\tDesigned and implemented an ETL pipeline in Python to clean and process large ecommerce customer datasets.\n",
        "•\tPerformed predictive analytics to identify highrisk churn customers, leading to targeted retention strategies and a measurable reduction in churn rates.\n",
        "3. Vestas Process Automation\n",
        "•\tAutomated employee data collection, analysis, and visualization processes using Power BI, Python, and Excel.\n",
        "•\tLeveraged Microsoft Power Query for streamlined internal data gathering, improving traceability and key metric visibility, significantly benefiting senior management decisionmaking.\n",
        "4.  SAP Data Automation\n",
        "•\tDeveloped automation solutions to streamline data management between Excel and SAP systems using Excel VBA and SAP automation features.\n",
        "•\tEnabled seamless updates of Excel data into SAP, enhancing operational efficiency and data accuracy across departments.\n",
        "5. Plastic Pollution Overview Dashboard\n",
        "•\tCreated a dynamic Tableau dashboard to visualize global plastic pollution metrics, including waste generation, recycling rates, and environmental impacts.\n",
        "•\tIntegrated multiple data sources and provided filters for regional and temporal analysis, offering actionable insights for stakeholders and supporting sustainability efforts.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "i3Ya1muycJh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2M477ZGcv_h"
      },
      "outputs": [],
      "source": [
        "# Desired Skill\n",
        "\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Set up the client\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional career coach and resume-writing assistant. Your task is to craft strong, impactful, and industry-relevant resume bullet points that make my profile stand out for data analytics, data engineering roles and other type of analyst roles\\n\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Step1: Extract all the keywords in the Job description which we need to use in resume. {jd} Step2: using the keyword extracted above and my resume  combine to generate an ideal resume for the job application.\n",
        "\n",
        "            Resume format:\n",
        "\n",
        "            *Work experience*\n",
        "            Company name\n",
        "            best suited 3 points with keywords\n",
        "\n",
        "            *Projects*\n",
        "            you can generate new projects with difficulty level easy when some mandatory keywords or technology is missing in my work experience\n",
        "            Title\n",
        "            short idea\n",
        "\n",
        "            {MyResume}\n",
        "            \"\"\"}\n",
        "\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "# Initialize a variable to store the output\n",
        "DesiredSkill = \"\"\n",
        "\n",
        "# Collect the response from the stream and store it\n",
        "for chunk in completion:\n",
        "    # Extract the content from the streamed chunk\n",
        "    content = chunk.choices[0].delta.content if chunk.choices[0].delta.content else \"\"\n",
        "    DesiredSkill += content  # Append to the variable\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DesiredSkill)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z00gWu9i0VQm",
        "outputId": "be585a57-fd22-4f8f-fc8c-daf3d342540d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the job description, I extracted the following keywords:\n",
            "\n",
            "* Data engineering\n",
            "* Data pipelines\n",
            "* Machine learning (ML)\n",
            "* Algorithm design\n",
            "* Software design\n",
            "* Concurrency\n",
            "* Data structures\n",
            "* Scalable distributed systems\n",
            "* Real-time data processing\n",
            "* Low-latency environment\n",
            "* Microservice architecture\n",
            "* Probabilistic algorithms\n",
            "* Data science\n",
            "* Collaborative environment\n",
            "* Problem-solving\n",
            "* Innovative thinking\n",
            "* Creativity\n",
            "\n",
            "Using these keywords, I combined them with your resume to generate an ideal resume for the Data Engineer position at StackAdapt:\n",
            "\n",
            "**Work Experience:**\n",
            "\n",
            "1. Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023)\n",
            "\t* Designed and implemented data-driven supplier evaluation models, utilizing machine learning algorithms to optimize procurement processes and reduce costs by 15%.\n",
            "\t* Developed and deployed real-time data pipelines to improve predictive analytics for demand forecasting, resulting in a 20% reduction in project timelines.\n",
            "\t* Collaborated with cross-functional teams to design and implement scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems.\n",
            "2. Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021)\n",
            "\t* Analyzed supplier costs and payment credit periods using probabilistic algorithms, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.\n",
            "\t* Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks.\n",
            "\t* Led RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs.\n",
            "3. Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021)\n",
            "\t* Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
            "\t* Built and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency.\n",
            "\t* Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
            "\n",
            "**Projects:**\n",
            "\n",
            "1. Real-Time Data Pipeline for E-commerce Analytics\n",
            "\t* Designed and implemented a real-time data pipeline using Apache Kafka, Apache Spark, and Apache Cassandra to process large e-commerce datasets.\n",
            "\t* Developed machine learning models to predict customer churn and recommend products, resulting in a 15% increase in sales.\n",
            "2. Scalable Distributed System for Image Classification\n",
            "\t* Designed and implemented a scalable distributed system using TensorFlow, Apache Spark, and Apache Hadoop to classify large images datasets.\n",
            "\t* Achieved a 90% accuracy rate in image classification, outperforming existing models.\n",
            "3. Low-Latency Algorithm for Real-Time Recommendations\n",
            "\t* Developed a low-latency algorithm using collaborative filtering and matrix factorization to provide real-time product recommendations.\n",
            "\t* Improved recommendation accuracy by 20% and reduced latency by 30%.\n",
            "\n",
            "**Education:**\n",
            "\n",
            "* Diploma in Data Analytics for Business, St. Clair College (2024)\n",
            "* Bachelor of Engineering, Mechanical, St. Joseph’s Institute of Technology (2024)\n",
            "\n",
            "**Certifications:**\n",
            "\n",
            "* SQL for Data Science (Coursera)\n",
            "* Python for Data Analysis (Udemy)\n",
            "* Tableau Desktop Specialist Certification\n",
            "\n",
            "Note: I generated new projects (1, 2, and 3) that incorporate the mandatory keywords and technologies mentioned in the job description, while highlighting your skills and experience in data engineering, machine learning, and software design. I also modified your work experience to emphasize your achievements and responsibilities in a way that aligns with the job requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DesiredSkills = \"\"\"Based on the job description, I extracted the following keywords:\n",
        "\n",
        "* Data engineering\n",
        "* Data pipelines\n",
        "* Machine learning (ML)\n",
        "* Algorithm design\n",
        "* Software design\n",
        "* Concurrency\n",
        "* Data structures\n",
        "* Scalable distributed systems\n",
        "* Real-time data processing\n",
        "* Low-latency environment\n",
        "* Microservice architecture\n",
        "* Probabilistic algorithms\n",
        "* Data science\n",
        "* Collaborative environment\n",
        "* Problem-solving\n",
        "* Innovative thinking\n",
        "* Creativity\n",
        "\n",
        "Using these keywords, I combined them with your resume to generate an ideal resume for the Data Engineer position at StackAdapt:\n",
        "\n",
        "**Work Experience:**\n",
        "\n",
        "1. Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023)\n",
        "\t* Designed and implemented data-driven supplier evaluation models, utilizing machine learning algorithms to optimize procurement processes and reduce costs by 15%.\n",
        "\t* Developed and deployed real-time data pipelines to improve predictive analytics for demand forecasting, resulting in a 20% reduction in project timelines.\n",
        "\t* Collaborated with cross-functional teams to design and implement scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems.\n",
        "2. Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021)\n",
        "\t* Analyzed supplier costs and payment credit periods using probabilistic algorithms, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.\n",
        "\t* Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks.\n",
        "\t* Led RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs.\n",
        "3. Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021)\n",
        "\t* Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
        "\t* Built and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency.\n",
        "\t* Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
        "\n",
        "**Projects:**\n",
        "\n",
        "1. Real-Time Data Pipeline for E-commerce Analytics\n",
        "\t* Designed and implemented a real-time data pipeline using Apache Kafka, Apache Spark, and Apache Cassandra to process large e-commerce datasets.\n",
        "\t* Developed machine learning models to predict customer churn and recommend products, resulting in a 15% increase in sales.\n",
        "2. Scalable Distributed System for Image Classification\n",
        "\t* Designed and implemented a scalable distributed system using TensorFlow, Apache Spark, and Apache Hadoop to classify large images datasets.\n",
        "\t* Achieved a 90% accuracy rate in image classification, outperforming existing models.\n",
        "3. Low-Latency Algorithm for Real-Time Recommendations\n",
        "\t* Developed a low-latency algorithm using collaborative filtering and matrix factorization to provide real-time product recommendations.\n",
        "\t* Improved recommendation accuracy by 20% and reduced latency by 30%.\n",
        "\n",
        "**Education:**\n",
        "\n",
        "* Diploma in Data Analytics for Business, St. Clair College (2024)\n",
        "* Bachelor of Engineering, Mechanical, St. Joseph’s Institute of Technology (2024)\n",
        "\n",
        "**Certifications:**\n",
        "\n",
        "* SQL for Data Science (Coursera)\n",
        "* Python for Data Analysis (Udemy)\n",
        "* Tableau Desktop Specialist Certification\n",
        "\n",
        "Note: I generated new projects (1, 2, and 3) that incorporate the mandatory keywords and technologies mentioned in the job description, while highlighting your skills and experience in data engineering, machine learning, and software design. I also modified your work experience to emphasize your achievements and responsibilities in a way that aligns with the job requirements.\"\"\""
      ],
      "metadata": {
        "id": "TNMfuFax78l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional career coach and resume-writing assistant. Your task is to craft strong, impactful, and industry-relevant resume bullet points that make my profile stand out for data analytics, data engineering roles and other type of analyst roles\\n\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{DesiredSkill}\\n\\n\\nPlease generate the updated resume sections in YAML format using the same structure as provided below. Make sure the output is valid YAML without any markdown formatting (i.e., no triple backticks) so it can be directly loaded by a YAML parser.\\n\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Initialize a variable to store the output\n",
        "llm_yaml = \"\"\n",
        "\n",
        "# Collect the response from the stream and store it\n",
        "for chunk in completion:\n",
        "    # Extract the content from the streamed chunk\n",
        "    content = chunk.choices[0].delta.content if chunk.choices[0].delta.content else \"\"\n",
        "    llm_yaml += content  # Append to the variable\n"
      ],
      "metadata": {
        "id": "z5F7_7t5zvXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJVi1Cd1Rac",
        "outputId": "45b1a7d0-9dc3-4370-e9e7-5072dfb4de72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work_experiences:\n",
            "  - company: Vestas Wind Technology, India\n",
            "    position: Procurement Engineer\n",
            "    dates: Nov 2021 - Aug 2023\n",
            "    achievements:\n",
            "      - Designed and implemented data-driven supplier evaluation models, utilizing machine learning algorithms to optimize procurement processes and reduce costs by 15%\n",
            "      - Developed and deployed real-time data pipelines to improve predictive analytics for demand forecasting, resulting in a 20% reduction in project timelines\n",
            "      - Collaborated with cross-functional teams to design and implement scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems\n",
            "  - company: Vestas, ManpowerGroup Services, India\n",
            "    position: Consultant ECM Purchaser\n",
            "    dates: Jan 2021 - Nov 2021\n",
            "    achievements:\n",
            "      - Analyzed supplier costs and payment credit periods using probabilistic algorithms, identifying potential savings of €3M and implementing standardized and efficient procurement strategies\n",
            "      - Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks\n",
            "      - Led RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs\n",
            "  - company: Valeo India, India\n",
            "    position: Procurement Engineer\n",
            "    dates: Jun 2018 - Jan 2021\n",
            "    achievements:\n",
            "      - Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies\n",
            "      - Built and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency\n",
            "      - Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually\n",
            "\n",
            "projects:\n",
            "  - title: Real-Time Data Pipeline for E-commerce Analytics\n",
            "    description: Designed and implemented a real-time data pipeline using Apache Kafka, Apache Spark, and Apache Cassandra to process large e-commerce datasets\n",
            "    achievements:\n",
            "      - Developed machine learning models to predict customer churn and recommend products, resulting in a 15% increase in sales\n",
            "  - title: Scalable Distributed System for Image Classification\n",
            "    description: Designed and implemented a scalable distributed system using TensorFlow, Apache Spark, and Apache Hadoop to classify large images datasets\n",
            "    achievements:\n",
            "      - Achieved a 90% accuracy rate in image classification, outperforming existing models\n",
            "  - title: Low-Latency Algorithm for Real-Time Recommendations\n",
            "    description: Developed a low-latency algorithm using collaborative filtering and matrix factorization to provide real-time product recommendations\n",
            "    achievements:\n",
            "      - Improved recommendation accuracy by 20% and reduced latency by 30%\n",
            "\n",
            "education:\n",
            "  - diploma: Diploma in Data Analytics for Business\n",
            "    institution: St. Clair College\n",
            "    date: 2024\n",
            "  - bachelor: Bachelor of Engineering, Mechanical\n",
            "    institution: St. Joseph’s Institute of Technology\n",
            "    date: 2024\n",
            "\n",
            "certifications:\n",
            "  - SQL for Data Science (Coursera)\n",
            "  - Python for Data Analysis (Udemy)\n",
            "  - Tableau Desktop Specialist Certification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading yaml"
      ],
      "metadata": {
        "id": "5bNknMr22vdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = yaml.safe_load(llm_yaml)"
      ],
      "metadata": {
        "id": "_2t7cYa253e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data"
      ],
      "metadata": {
        "id": "XJOasVQv54fS",
        "outputId": "a4cfad9b-8ba1-447e-c949-0f2a749d7ed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'work_experiences': [{'company': 'Vestas Wind Technology, India',\n",
              "   'position': 'Procurement Engineer',\n",
              "   'dates': 'Nov 2021 - Aug 2023',\n",
              "   'achievements': ['Designed and implemented data-driven supplier evaluation models, utilizing machine learning algorithms to optimize procurement processes and reduce costs by 15%',\n",
              "    'Developed and deployed real-time data pipelines to improve predictive analytics for demand forecasting, resulting in a 20% reduction in project timelines',\n",
              "    'Collaborated with cross-functional teams to design and implement scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems']},\n",
              "  {'company': 'Vestas, ManpowerGroup Services, India',\n",
              "   'position': 'Consultant ECM Purchaser',\n",
              "   'dates': 'Jan 2021 - Nov 2021',\n",
              "   'achievements': ['Analyzed supplier costs and payment credit periods using probabilistic algorithms, identifying potential savings of €3M and implementing standardized and efficient procurement strategies',\n",
              "    'Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks',\n",
              "    'Led RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs']},\n",
              "  {'company': 'Valeo India, India',\n",
              "   'position': 'Procurement Engineer',\n",
              "   'dates': 'Jun 2018 - Jan 2021',\n",
              "   'achievements': ['Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies',\n",
              "    'Built and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency',\n",
              "    'Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually']}],\n",
              " 'projects': [{'title': 'Real-Time Data Pipeline for E-commerce Analytics',\n",
              "   'description': 'Designed and implemented a real-time data pipeline using Apache Kafka, Apache Spark, and Apache Cassandra to process large e-commerce datasets',\n",
              "   'achievements': ['Developed machine learning models to predict customer churn and recommend products, resulting in a 15% increase in sales']},\n",
              "  {'title': 'Scalable Distributed System for Image Classification',\n",
              "   'description': 'Designed and implemented a scalable distributed system using TensorFlow, Apache Spark, and Apache Hadoop to classify large images datasets',\n",
              "   'achievements': ['Achieved a 90% accuracy rate in image classification, outperforming existing models']},\n",
              "  {'title': 'Low-Latency Algorithm for Real-Time Recommendations',\n",
              "   'description': 'Developed a low-latency algorithm using collaborative filtering and matrix factorization to provide real-time product recommendations',\n",
              "   'achievements': ['Improved recommendation accuracy by 20% and reduced latency by 30%']}],\n",
              " 'education': [{'diploma': 'Diploma in Data Analytics for Business',\n",
              "   'institution': 'St. Clair College',\n",
              "   'date': 2024},\n",
              "  {'bachelor': 'Bachelor of Engineering, Mechanical',\n",
              "   'institution': 'St. Joseph’s Institute of Technology',\n",
              "   'date': 2024}],\n",
              " 'certifications': ['SQL for Data Science (Coursera)',\n",
              "  'Python for Data Analysis (Udemy)',\n",
              "  'Tableau Desktop Specialist Certification']}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "\n",
        "# Load your existing YAML file\n",
        "yaml_file_path = '/content/pdf-build/test2.yaml'\n",
        "with open(yaml_file_path, 'r') as f:\n",
        "    resume_data = yaml.safe_load(f)\n",
        "\n",
        "# Load the new YAML content from the LLM output\n",
        "new_data = yaml.safe_load(llm_yaml)\n",
        "\n",
        "# Update specific parts of your resume, e.g., work and projects sections\n",
        "resume_data['work'] = new_data.get('work', resume_data.get('work'))\n",
        "resume_data['projects'] = new_data.get('projects', resume_data.get('projects'))\n",
        "\n",
        "# Write the updated YAML back to the file\n",
        "with open(yaml_file_path, 'w') as f:\n",
        "    yaml.safe_dump(resume_data, f, default_flow_style=False)\n",
        "\n",
        "print(\"YAML file updated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71arXCTp3Mrd",
        "outputId": "48dad52b-c973-47d6-9ef4-51436cbf2f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Define the path to your existing YAML file\n",
        "yaml_file_path = \"/content/pdf-build/test2.yaml\"\n",
        "\n",
        "# Load the existing YAML data\n",
        "with open(yaml_file_path, \"r\") as f:\n",
        "    resume_data = yaml.safe_load(f)\n",
        "\n",
        "# New YAML content from the LLM output (replace this with actual LLM output)\n",
        "llm_yaml = \"\"\"\n",
        "work:\n",
        "- name: Vestas Wind Technology, India\n",
        "  position: Procurement Specialist\n",
        "  startDate: '2021-11-01'\n",
        "  endDate: '2023-08-01'\n",
        "  highlights:\n",
        "  - Achieved a 15% reduction in procurement costs by developing Python-based automation tools for workflows.\n",
        "  - Improved supplier evaluation using advanced SQL and Power BI dashboards.\n",
        "  - Integrated ML Flow and Python-based tools to automate RFQ processes, reducing processing time by 40%.\n",
        "\n",
        "- name: ManpowerGroup Services, India\n",
        "  position: Consultant ECM Purchaser, Vestas\n",
        "  startDate: '2021-01-01'\n",
        "  endDate: '2021-11-01'\n",
        "  highlights:\n",
        "  - Developed predictive analytics models for demand forecasting, improving delivery timelines by 20%.\n",
        "  - Automated internal workflows using Python and Power Query, enhancing team efficiency by 30%.\n",
        "  - Designed a procurement dashboard using Power BI and Cloud Storage.\n",
        "\n",
        "- name: Valeo India\n",
        "  position: Procurement Engineer\n",
        "  startDate: '2018-06-01'\n",
        "  endDate: '2021-01-01'\n",
        "  highlights:\n",
        "  - Achieved 15% cost reductions through strategic vendor sourcing and procurement strategies.\n",
        "  - Built Power BI dashboards for real-time KPI tracking, reducing reporting time by 40%.\n",
        "  - Developed Python-based tools for cost analysis, saving $500K annually.\n",
        "\n",
        "projects:\n",
        "- name: LLM-Powered Insights Generator\n",
        "  description: Built a retrieval-augmented generation (RAG) solution using LangChain and Azure AISearch for advanced data insights. Developed agentic workflows (GraphRAG) to streamline large-scale report generation, reducing manual efforts by 50%.\n",
        "  keywords:\n",
        "  - LangChain\n",
        "  - Azure AISearch\n",
        "  - GraphRAG\n",
        "\n",
        "- name: Meta Financial Dashboard\n",
        "  description: Designed a Tableau dashboard analyzing KPIs like P/E Ratio and Debt to Equity, improving decision-making accuracy for stakeholders.\n",
        "  keywords:\n",
        "  - Tableau\n",
        "  - Financial Analytics\n",
        "\n",
        "- name: Customer Churn Prediction\n",
        "  description: Implemented ML algorithms to predict customer churn using Python and Power BI, integrating data transformation workflows with ML Flow.\n",
        "  keywords:\n",
        "  - Python\n",
        "  - Power BI\n",
        "  - ML Flow\n",
        "\"\"\"\n",
        "\n",
        "# Load the new YAML content\n",
        "new_data = yaml.safe_load(llm_yaml)\n",
        "\n",
        "# Update specific sections in the resume YAML\n",
        "resume_data[\"work\"] = new_data.get(\"work\", resume_data.get(\"work\"))\n",
        "resume_data[\"projects\"] = new_data.get(\"projects\", resume_data.get(\"projects\"))\n",
        "\n",
        "# Save the updated YAML back to the file\n",
        "with open(yaml_file_path, \"w\") as f:\n",
        "    yaml.safe_dump(resume_data, f, default_flow_style=False, allow_unicode=True)\n",
        "\n",
        "print(\"YAML file updated successfully!\")\n"
      ],
      "metadata": {
        "id": "3PIfcYPD88zA",
        "outputId": "e0d35ab0-8a0c-4014-9286-f243b474da11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML file updated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q weasyprint\n",
        "!apt-get -qq install -y libpango-1.0-0 libpangocairo-1.0-0"
      ],
      "metadata": {
        "id": "IB1QNSUj5h1a",
        "outputId": "0d3c89d2-381c-48ba-b7e7-2ff54b43f34b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/301.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.9/301.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pdf-build/resumy.py build -o test2.pdf /content/pdf-build/test2.yaml"
      ],
      "metadata": {
        "id": "RMBXeSyf5Vag",
        "outputId": "6a60f3c8-c6f7-41b0-e35b-1702667c4626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated HTML content:\n",
            "Attempting to create resume PDF at: test2.pdf\n",
            "the directory is \n",
            "SUCCESS: PDF created at test2.pdf\n"
          ]
        }
      ]
    }
  ]
}
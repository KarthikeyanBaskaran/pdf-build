{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthikeyanBaskaran/pdf-build/blob/main/Resume_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6rCgqRLsZqUs"
      },
      "outputs": [],
      "source": [
        "# prompt: how can i pip install without output steps\n",
        "\n",
        "!pip install --quiet groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPcFmp3B7QJD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKt6eA3dz5yW",
        "outputId": "2d20d3f1-58ba-4640-f410-f8333cafec03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! apt-get install git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWKz0kvz8yY",
        "outputId": "7ebf8ab8-7238-4167-d521-a93bf4d4e225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pdf-build'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/39)\u001b[K\rremote: Counting objects:   5% (2/39)\u001b[K\rremote: Counting objects:   7% (3/39)\u001b[K\rremote: Counting objects:  10% (4/39)\u001b[K\rremote: Counting objects:  12% (5/39)\u001b[K\rremote: Counting objects:  15% (6/39)\u001b[K\rremote: Counting objects:  17% (7/39)\u001b[K\rremote: Counting objects:  20% (8/39)\u001b[K\rremote: Counting objects:  23% (9/39)\u001b[K\rremote: Counting objects:  25% (10/39)\u001b[K\rremote: Counting objects:  28% (11/39)\u001b[K\rremote: Counting objects:  30% (12/39)\u001b[K\rremote: Counting objects:  33% (13/39)\u001b[K\rremote: Counting objects:  35% (14/39)\u001b[K\rremote: Counting objects:  38% (15/39)\u001b[K\rremote: Counting objects:  41% (16/39)\u001b[K\rremote: Counting objects:  43% (17/39)\u001b[K\rremote: Counting objects:  46% (18/39)\u001b[K\rremote: Counting objects:  48% (19/39)\u001b[K\rremote: Counting objects:  51% (20/39)\u001b[K\rremote: Counting objects:  53% (21/39)\u001b[K\rremote: Counting objects:  56% (22/39)\u001b[K\rremote: Counting objects:  58% (23/39)\u001b[K\rremote: Counting objects:  61% (24/39)\u001b[K\rremote: Counting objects:  64% (25/39)\u001b[K\rremote: Counting objects:  66% (26/39)\u001b[K\rremote: Counting objects:  69% (27/39)\u001b[K\rremote: Counting objects:  71% (28/39)\u001b[K\rremote: Counting objects:  74% (29/39)\u001b[K\rremote: Counting objects:  76% (30/39)\u001b[K\rremote: Counting objects:  79% (31/39)\u001b[K\rremote: Counting objects:  82% (32/39)\u001b[K\rremote: Counting objects:  84% (33/39)\u001b[K\rremote: Counting objects:  87% (34/39)\u001b[K\rremote: Counting objects:  89% (35/39)\u001b[K\rremote: Counting objects:  92% (36/39)\u001b[K\rremote: Counting objects:  94% (37/39)\u001b[K\rremote: Counting objects:  97% (38/39)\u001b[K\rremote: Counting objects: 100% (39/39)\u001b[K\rremote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/36)\u001b[K\rremote: Compressing objects:   5% (2/36)\u001b[K\rremote: Compressing objects:   8% (3/36)\u001b[K\rremote: Compressing objects:  11% (4/36)\u001b[K\rremote: Compressing objects:  13% (5/36)\u001b[K\rremote: Compressing objects:  16% (6/36)\u001b[K\rremote: Compressing objects:  19% (7/36)\u001b[K\rremote: Compressing objects:  22% (8/36)\u001b[K\rremote: Compressing objects:  25% (9/36)\u001b[K\rremote: Compressing objects:  27% (10/36)\u001b[K\rremote: Compressing objects:  30% (11/36)\u001b[K\rremote: Compressing objects:  33% (12/36)\u001b[K\rremote: Compressing objects:  36% (13/36)\u001b[K\rremote: Compressing objects:  38% (14/36)\u001b[K\rremote: Compressing objects:  41% (15/36)\u001b[K\rremote: Compressing objects:  44% (16/36)\u001b[K\rremote: Compressing objects:  47% (17/36)\u001b[K\rremote: Compressing objects:  50% (18/36)\u001b[K\rremote: Compressing objects:  52% (19/36)\u001b[K\rremote: Compressing objects:  55% (20/36)\u001b[K\rremote: Compressing objects:  58% (21/36)\u001b[K\rremote: Compressing objects:  61% (22/36)\u001b[K\rremote: Compressing objects:  63% (23/36)\u001b[K\rremote: Compressing objects:  66% (24/36)\u001b[K\rremote: Compressing objects:  69% (25/36)\u001b[K\rremote: Compressing objects:  72% (26/36)\u001b[K\rremote: Compressing objects:  75% (27/36)\u001b[K\rremote: Compressing objects:  77% (28/36)\u001b[K\rremote: Compressing objects:  80% (29/36)\u001b[K\rremote: Compressing objects:  83% (30/36)\u001b[K\rremote: Compressing objects:  86% (31/36)\u001b[K\rremote: Compressing objects:  88% (32/36)\u001b[K\rremote: Compressing objects:  91% (33/36)\u001b[K\rremote: Compressing objects:  94% (34/36)\u001b[K\rremote: Compressing objects:  97% (35/36)\u001b[K\rremote: Compressing objects: 100% (36/36)\u001b[K\rremote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "Receiving objects:   2% (1/39)\rReceiving objects:   5% (2/39)\rReceiving objects:   7% (3/39)\rReceiving objects:  10% (4/39)\rReceiving objects:  12% (5/39)\rReceiving objects:  15% (6/39)\rReceiving objects:  17% (7/39)\rReceiving objects:  20% (8/39)\rReceiving objects:  23% (9/39)\rReceiving objects:  25% (10/39)\rReceiving objects:  28% (11/39)\rReceiving objects:  30% (12/39)\rReceiving objects:  33% (13/39)\rReceiving objects:  35% (14/39)\rReceiving objects:  38% (15/39)\rReceiving objects:  41% (16/39)\rReceiving objects:  43% (17/39)\rReceiving objects:  46% (18/39)\rremote: Total 39 (delta 19), reused 12 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  48% (19/39)\rReceiving objects:  51% (20/39)\rReceiving objects:  53% (21/39)\rReceiving objects:  56% (22/39)\rReceiving objects:  58% (23/39)\rReceiving objects:  61% (24/39)\rReceiving objects:  64% (25/39)\rReceiving objects:  66% (26/39)\rReceiving objects:  69% (27/39)\rReceiving objects:  71% (28/39)\rReceiving objects:  74% (29/39)\rReceiving objects:  76% (30/39)\rReceiving objects:  79% (31/39)\rReceiving objects:  82% (32/39)\rReceiving objects:  84% (33/39)\rReceiving objects:  87% (34/39)\rReceiving objects:  89% (35/39)\rReceiving objects:  92% (36/39)\rReceiving objects:  94% (37/39)\rReceiving objects:  97% (38/39)\rReceiving objects: 100% (39/39)\rReceiving objects: 100% (39/39), 107.29 KiB | 15.33 MiB/s, done.\n",
            "Resolving deltas:   0% (0/19)\rResolving deltas:   5% (1/19)\rResolving deltas:  10% (2/19)\rResolving deltas:  15% (3/19)\rResolving deltas:  21% (4/19)\rResolving deltas:  26% (5/19)\rResolving deltas:  31% (6/19)\rResolving deltas:  36% (7/19)\rResolving deltas:  42% (8/19)\rResolving deltas:  47% (9/19)\rResolving deltas:  52% (10/19)\rResolving deltas:  57% (11/19)\rResolving deltas:  63% (12/19)\rResolving deltas:  68% (13/19)\rResolving deltas:  73% (14/19)\rResolving deltas:  78% (15/19)\rResolving deltas:  84% (16/19)\rResolving deltas:  89% (17/19)\rResolving deltas:  94% (18/19)\rResolving deltas: 100% (19/19)\rResolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KarthikeyanBaskaran/pdf-build.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "jd = input('Enter the job description here')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWmmCBGhQ3Fv"
      },
      "outputs": [],
      "source": [
        "# jd = \"\"\"\n",
        "# StackAdapt is a self-serve advertising platform that specializes in multi-channel solutions including native, display, video, connected TV, audio, in-game, and digital out-of-home ads. We empower hundreds of digitally-focused companies to deliver outcomes and exceptional campaign performance everyday. StackAdapt was founded with a vision to be more than an advertising platform, it’s a hub of innovation, imagination and creativity.\n",
        "\n",
        "# We're looking to add Data Engineers to our data science team! This team works on solving complex problems for StackAdapt's digital advertising platform. You'll be working directly with our data scientists, data engineers, Engineering team, and CTO on building pipelines and ad optimization models. With databases that process millions of requests per second, there's no shortage of data and problems to tackle.\n",
        "\n",
        "# Want to learn more about our Data Science Team: https://alldus.com/ie/blog/podcasts/aiinaction-ned-dimitrov-stackadapt/\n",
        "\n",
        "# Learn more about our team culture here: https://www.stackadapt.com/careers/data-science\n",
        "\n",
        "# Watch our talk at Amazon Tech Talks: https://www.youtube.com/watch?v=lRqu-a4gPuU\n",
        "\n",
        "# StackAdapt is a Remote First company, and we are open to candidates in various of our operating locations throughout the globe!\n",
        "\n",
        "# What you'll be doing:\n",
        "\n",
        "# Design modular and scalable real time data pipelines to handle huge datasets\n",
        "# Understand and implement custom ML algorithms in a low latency environment\n",
        "# Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently\n",
        "\n",
        "# What you'll bring to the table:\n",
        "\n",
        "# Have the ability to take an ambiguously defined task, and break it down into actionable steps\n",
        "# Have deep understanding of algorithm and software design, concurrency, and data structures\n",
        "# Experience in implementing probabilistic or machine learning algorithms\n",
        "# Interest in designing scalable distributed systems\n",
        "# A high GPA from a well-respected Computer Science program\n",
        "# Enjoy working in a friendly, collaborative environment with others\n",
        "\n",
        "# StackAdapters enjoy:\n",
        "\n",
        "# Competitive salary + equity\n",
        "# RRSP matching\n",
        "# 3 weeks vacation + 3 personal care days + 1 Culture & Belief day + birthdays off\n",
        "# Access to a comprehensive mental health care platform\n",
        "# Health benefits from day one of employment\n",
        "# Work from home reimbursements\n",
        "# Optional global WeWork membership for those who want a change from their home office\n",
        "# Robust training and onboarding program\n",
        "# Coverage and support of personal development initiatives (conferences, courses, etc)\n",
        "# Access to StackAdapt programmatic courses and certifications to support continuous learning\n",
        "# Mentorship opportunities with industry leaders\n",
        "# An awesome parental leave policy\n",
        "# A friendly, welcoming, and supportive culture\n",
        "# Our social and team events!\n",
        "\n",
        "# If this role speaks to you then please apply - we'd love to speak with you. Due to a high volume of interest, only those shortlisted for interview will be contacted.\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i3Ya1muycJh3"
      },
      "outputs": [],
      "source": [
        "MyResume = \"\"\"Professional Experience:\n",
        "1. Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023):\n",
        "•\tAchieved a 15% reduction in procurement costs by developing a data-driven supplier evaluation model and conducting detailed cost analysis.\n",
        "•\tImproved project timelines by 20% through predictive analytics for demand forecasting and proactive delay management.\n",
        "•\tSaved $85M annually by developing and implementing Python-based automation tools, streamlining procurement workflows and enhancing operational efficiency.\n",
        "•\tEnhanced supplier performance monitoring by designing vendor scorecards using Excel and Power BI, enabling data visualization for better decision-making.\n",
        "•\tIntegrated IT systems into procurement processes, deploying Microsoft Power Query and SharePoint to automate reporting and improve data accuracy, reducing manual efforts by 30%.\n",
        "•\tDeveloped a procurement dashboard in Power BI to track KPIs, supplier performance, and category spend, enabling leadership to make informed decisions.\n",
        "•\tOptimized material master data accuracy through automation, improving data consistency and reducing errors by 25%.\n",
        "•\tCollaborated with IT and cross-functional teams to develop scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems.\n",
        "•\tImplemented risk mitigation strategies by analyzing supply chain data and identifying alternative sourcing options to address potential delivery bottlenecks.\n",
        "•\tStreamlined RFQ processes using advanced quote comparison tools and automation, reducing processing time by 40%.\n",
        "•\tNegotiated supplier contracts to achieve cost savings and ensure compliance with government policies, leveraging IT tools for real-time data tracking.\n",
        "•\tDesigned and deployed customized procurement strategies to handle delivery complexities, ensuring continuity in production and minimizing downtime.\n",
        "•\tImproved KPI tracking by automating SLA adherence reporting, increasing visibility for stakeholders and reducing manual intervention.\n",
        "•\tLed IT-driven innovations in procurement by identifying opportunities for automation and implementing solutions that enhanced scalability and flexibility across procurement functions.\n",
        "\n",
        "2. Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021):\n",
        "•\tReduced production costs by 10% through process optimization analyses, utilizing data-driven insights to streamline operations.\n",
        "•\tIdentified potential savings of €3M by analyzing supplier costs and payment credit periods, implementing standardized and efficient procurement strategies.\n",
        "•\tImproved operational efficiency by 15% by leading cross-functional collaboration and deploying Lean methodologies.\n",
        "•\tEnhanced project delivery capacity by 20% through process improvements that freed up resources for handling additional projects.\n",
        "•\tDeveloped and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks.\n",
        "•\tLed RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs.\n",
        "•\tCollaborated with product management and design teams to define project scope and requirements, ensuring alignment with customer needs and delivering projects 10% faster.\n",
        "•\tBuilt Python-based automation scripts, optimizing internal workflows and reducing manual effort by 25%, enhancing team efficiency.\n",
        "•\tStrengthened stakeholder relationships by implementing efficient communication strategies, improving supplier and sponsor engagement by 12%.\n",
        "•\tDeveloped dashboards and reporting tools using Python and cloud services to monitor KPIs, improving decision-making accuracy by 18%.\n",
        "\n",
        "3. Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021):\n",
        "•\tManaged RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
        "•\tBuilt and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency.\n",
        "•\tExecuted supplier auctions using digital platforms, reducing overall part costs by 8-12% while ensuring competitive pricing.\n",
        "•\tPerformed zero-based costing using advanced Excel (macros, pivot tables) and analytical tools, enabling 15% cost savings through data-driven negotiations.\n",
        "•\tDeveloped Power BI dashboards to track procurement KPIs, cutting reporting time by 40% and enhancing supplier performance monitoring.\n",
        "•\tUtilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
        "•\tIntegrated ERP tools (SAP) to streamline procurement workflows, reducing manual effort by 30% and enhancing process accuracy.\n",
        "•\tSourced globally from cost-effective suppliers to meet production demands, ensuring 100% on-time delivery and adherence to quality standards.\n",
        "•\tGained expertise in manufacturing techniques, achieving a 10% reduction in project costs by aligning processes with best-cost practices.\n",
        "•\tCollaborated on ERP system enhancements, reducing procurement cycle times by 20% and improving data accessibility for stakeholders.\n",
        "\n",
        "\n",
        "Education:\n",
        " Diploma in Data Analytics for Business, St. Clair College (2024):\n",
        " GPA: 4.0\n",
        "    Relevant coursework: Data Mining, Predictive Analytics, Database Management, and Machine Learning.\n",
        "\n",
        " Bachelor of Engineering, Mechanical,  St. Joseph’s institute of Technology (2024):\n",
        "   GPA: 3.5\n",
        "\n",
        "Certifications:\n",
        " SQL for Data Science (Coursera)\n",
        " Python for Data Analysis (Udemy)\n",
        " Tableau Desktop Specialist Certification\n",
        "\n",
        "#### Projects:\n",
        "1. Meta Financial Dashboard\n",
        "•\tDeveloped an interactive Tableau dashboard to analyze key financial KPIs such as P/E Ratio, Current Ratio, and Debt to Equity.\n",
        "•\tDelivered actionable insights that improved financial decisionmaking, enhancing reporting accuracy and strategic planning.\n",
        "2. Customer Churn Analysis\n",
        "•\tDesigned and implemented an ETL pipeline in Python to clean and process large ecommerce customer datasets.\n",
        "•\tPerformed predictive analytics to identify highrisk churn customers, leading to targeted retention strategies and a measurable reduction in churn rates.\n",
        "3. Vestas Process Automation\n",
        "•\tAutomated employee data collection, analysis, and visualization processes using Power BI, Python, and Excel.\n",
        "•\tLeveraged Microsoft Power Query for streamlined internal data gathering, improving traceability and key metric visibility, significantly benefiting senior management decisionmaking.\n",
        "4.  SAP Data Automation\n",
        "•\tDeveloped automation solutions to streamline data management between Excel and SAP systems using Excel VBA and SAP automation features.\n",
        "•\tEnabled seamless updates of Excel data into SAP, enhancing operational efficiency and data accuracy across departments.\n",
        "5. Plastic Pollution Overview Dashboard\n",
        "•\tCreated a dynamic Tableau dashboard to visualize global plastic pollution metrics, including waste generation, recycling rates, and environmental impacts.\n",
        "•\tIntegrated multiple data sources and provided filters for regional and temporal analysis, offering actionable insights for stakeholders and supporting sustainability efforts.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R2M477ZGcv_h"
      },
      "outputs": [],
      "source": [
        "# Desired Skill\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "try: \n",
        "    from google.colab import userdata\n",
        "    client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")\n",
        "except:\n",
        "    client = Groq(\n",
        "    api_key='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')\n",
        "\n",
        "# Set up the client\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional career coach and resume-writing assistant. Your task is to craft strong, impactful, and industry-relevant resume bullet points that make my profile stand out for data analytics, data engineering roles and other type of analyst roles\\n\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Step1: Extract all the keywords in the Job description which we need to use in resume. {jd} Step2: using the keyword extracted above and my resume  combine to generate an ideal resume for the job application.\n",
        "\n",
        "            Resume format:\n",
        "\n",
        "            *Work experience*\n",
        "            Company name\n",
        "            best suited 3 points with keywords\n",
        "\n",
        "            *Projects*\n",
        "            you can generate new projects with difficulty level easy when some mandatory keywords or technology is missing in my work experience\n",
        "            Title\n",
        "            short idea\n",
        "\n",
        "            {MyResume}\n",
        "            \"\"\"}\n",
        "\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "# Initialize a variable to store the output\n",
        "DesiredSkill = \"\"\n",
        "\n",
        "# Collect the response from the stream and store it\n",
        "for chunk in completion:\n",
        "    # Extract the content from the streamed chunk\n",
        "    content = chunk.choices[0].delta.content if chunk.choices[0].delta.content else \"\"\n",
        "    DesiredSkill += content  # Append to the variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z00gWu9i0VQm",
        "outputId": "be585a57-fd22-4f8f-fc8c-daf3d342540d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the job description and the keywords extracted, I have crafted an ideal resume format for the Data (Python) Developer role at DarkVision:\n",
            "\n",
            "**Work Experience:**\n",
            "\n",
            "1. **Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023)**\n",
            "\t* Developed and maintained Python-based automation tools to streamline procurement workflows, resulting in a 15% reduction in procurement costs and a 20% improvement in project timelines.\n",
            "\t* Designed and deployed Power BI dashboards to track KPIs, supplier performance, and category spend, enabling data-driven decision-making and a 25% reduction in manual efforts.\n",
            "\t* Utilized predictive analytics and data visualization to optimize material master data accuracy, reducing errors by 25% and improving data consistency.\n",
            "2. **Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021)**\n",
            "\t* Analyzed supplier costs and payment credit periods using data-driven insights, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.\n",
            "\t* Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and a 20% improvement in operational efficiency.\n",
            "\t* Built Python-based automation scripts to optimize internal workflows, reducing manual effort by 25% and enhancing team efficiency.\n",
            "3. **Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021)**\n",
            "\t* Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
            "\t* Developed Power BI dashboards to track procurement KPIs, cutting reporting time by 40% and enhancing supplier performance monitoring.\n",
            "\t* Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
            "\n",
            "**Projects:**\n",
            "\n",
            "1. **Meta Financial Dashboard**\n",
            "\t* Developed an interactive Tableau dashboard to analyze key financial KPIs, delivering actionable insights and improving financial decision-making.\n",
            "2. **Customer Churn Analysis**\n",
            "\t* Designed and implemented an ETL pipeline in Python to clean and process large ecommerce customer datasets, performing predictive analytics to identify high-risk churn customers.\n",
            "3. **Vestas Process Automation**\n",
            "\t* Automated employee data collection, analysis, and visualization processes using Power BI, Python, and Excel, improving traceability and key metric visibility.\n",
            "4. **SAP Data Automation**\n",
            "\t* Developed automation solutions to streamline data management between Excel and SAP systems, enabling seamless updates of Excel data into SAP and enhancing operational efficiency.\n",
            "5. **3D Data Visualization Project** (new project)\n",
            "\t* Developed a 3D data visualization tool using Python and Matplotlib to represent and visualize complex data, generating new customer deliverables and improving data analysis capabilities.\n",
            "6. **Image Processing Project** (new project)\n",
            "\t* Developed an image processing tool using Python and OpenCV to analyze and visualize image data, applying techniques such as object detection and image segmentation to improve data analysis capabilities.\n",
            "\n",
            "**Education:**\n",
            "\n",
            "* Diploma in Data Analytics for Business, St. Clair College (2024)\n",
            "* Bachelor of Engineering, Mechanical, St. Joseph’s Institute of Technology (2024)\n",
            "\n",
            "**Certifications:**\n",
            "\n",
            "* SQL for Data Science (Coursera)\n",
            "* Python for Data Analysis (Udemy)\n",
            "* Tableau Desktop Specialist Certification\n",
            "\n",
            "I have added two new projects, **3D Data Visualization Project** and **Image Processing Project**, to demonstrate expertise in 3D data visualization and image processing, which are relevant to the job description. I have also highlighted the use of Python, data visualization, and data analysis in the work experience and project sections to match the job requirements.\n"
          ]
        }
      ],
      "source": [
        "print(DesiredSkill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNMfuFax78l8"
      },
      "outputs": [],
      "source": [
        "# DesiredSkills = \"\"\"Based on the job description, I extracted the following keywords:\n",
        "\n",
        "# * Data engineering\n",
        "# * Data pipelines\n",
        "# * Machine learning (ML)\n",
        "# * Algorithm design\n",
        "# * Software design\n",
        "# * Concurrency\n",
        "# * Data structures\n",
        "# * Scalable distributed systems\n",
        "# * Real-time data processing\n",
        "# * Low-latency environment\n",
        "# * Microservice architecture\n",
        "# * Probabilistic algorithms\n",
        "# * Data science\n",
        "# * Collaborative environment\n",
        "# * Problem-solving\n",
        "# * Innovative thinking\n",
        "# * Creativity\n",
        "\n",
        "# Using these keywords, I combined them with your resume to generate an ideal resume for the Data Engineer position at StackAdapt:\n",
        "\n",
        "# **Work Experience:**\n",
        "\n",
        "# 1. Procurement Engineer, Vestas Wind Technology, India (Nov 2021 – Aug 2023)\n",
        "# \t* Designed and implemented data-driven supplier evaluation models, utilizing machine learning algorithms to optimize procurement processes and reduce costs by 15%.\n",
        "# \t* Developed and deployed real-time data pipelines to improve predictive analytics for demand forecasting, resulting in a 20% reduction in project timelines.\n",
        "# \t* Collaborated with cross-functional teams to design and implement scalable software solutions for supply chain monitoring, ensuring seamless integration with existing systems.\n",
        "# 2. Consultant ECM Purchaser, Vestas, ManpowerGroup Services, India (Jan 2021 – Nov 2021)\n",
        "# \t* Analyzed supplier costs and payment credit periods using probabilistic algorithms, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.\n",
        "# \t* Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and faster completion of routine tasks.\n",
        "# \t* Led RFx processes across wind turbine segments, negotiating competitive pricing with global vendors and achieving an 8% reduction in procurement costs.\n",
        "# 3. Procurement Engineer, Valeo India, India (Jun 2018 – Jan 2021)\n",
        "# \t* Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
        "# \t* Built and maintained a robust supplier base by inducting vendors, implementing double sourcing, and driving localization, resulting in a 20% improvement in supply chain efficiency.\n",
        "# \t* Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
        "\n",
        "# **Projects:**\n",
        "\n",
        "# 1. Real-Time Data Pipeline for E-commerce Analytics\n",
        "# \t* Designed and implemented a real-time data pipeline using Apache Kafka, Apache Spark, and Apache Cassandra to process large e-commerce datasets.\n",
        "# \t* Developed machine learning models to predict customer churn and recommend products, resulting in a 15% increase in sales.\n",
        "# 2. Scalable Distributed System for Image Classification\n",
        "# \t* Designed and implemented a scalable distributed system using TensorFlow, Apache Spark, and Apache Hadoop to classify large images datasets.\n",
        "# \t* Achieved a 90% accuracy rate in image classification, outperforming existing models.\n",
        "# 3. Low-Latency Algorithm for Real-Time Recommendations\n",
        "# \t* Developed a low-latency algorithm using collaborative filtering and matrix factorization to provide real-time product recommendations.\n",
        "# \t* Improved recommendation accuracy by 20% and reduced latency by 30%.\n",
        "\n",
        "# **Education:**\n",
        "\n",
        "# * Diploma in Data Analytics for Business, St. Clair College (2024)\n",
        "# * Bachelor of Engineering, Mechanical, St. Joseph’s Institute of Technology (2024)\n",
        "\n",
        "# **Certifications:**\n",
        "\n",
        "# * SQL for Data Science (Coursera)\n",
        "# * Python for Data Analysis (Udemy)\n",
        "# * Tableau Desktop Specialist Certification\n",
        "\n",
        "# Note: I generated new projects (1, 2, and 3) that incorporate the mandatory keywords and technologies mentioned in the job description, while highlighting your skills and experience in data engineering, machine learning, and software design. I also modified your work experience to emphasize your achievements and responsibilities in a way that aligns with the job requirements.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "z5F7_7t5zvXr"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "\n",
        "try: \n",
        "    from google.colab import userdata\n",
        "    client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")\n",
        "except:\n",
        "    client = Groq(\n",
        "    api_key='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')\n",
        "    \n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional career coach and resume-writing assistant. Your task is to craft strong, impactful, and industry-relevant resume bullet points that make my profile stand out for data analytics, data engineering roles and other type of analyst roles\\n\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{DesiredSkill}\\n\\n\\nPlease generate the updated resume sections in YAML format using the same structure as provided below. Make sure the output is valid YAML without any markdown formatting (i.e., no triple backticks) so it can be directly loaded by a YAML parser. i also need keywords from the project as a side heading in projects\\n\\n\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Initialize a variable to store the output\n",
        "llm_yaml = \"\"\n",
        "\n",
        "# Collect the response from the stream and store it\n",
        "for chunk in completion:\n",
        "    # Extract the content from the streamed chunk\n",
        "    content = chunk.choices[0].delta.content if chunk.choices[0].delta.content else \"\"\n",
        "    llm_yaml += content  # Append to the variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJVi1Cd1Rac",
        "outputId": "45b1a7d0-9dc3-4370-e9e7-5072dfb4de72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "work_experience:\n",
            "  - job_title: Procurement Engineer\n",
            "    company: Vestas Wind Technology, India\n",
            "    duration: Nov 2021 - Aug 2023\n",
            "    achievements:\n",
            "      - Developed and maintained Python-based automation tools to streamline procurement workflows, resulting in a 15% reduction in procurement costs and a 20% improvement in project timelines.\n",
            "      - Designed and deployed Power BI dashboards to track KPIs, supplier performance, and category spend, enabling data-driven decision-making and a 25% reduction in manual efforts.\n",
            "      - Utilized predictive analytics and data visualization to optimize material master data accuracy, reducing errors by 25% and improving data consistency.\n",
            "  - job_title: Consultant ECM Purchaser\n",
            "    company: Vestas, ManpowerGroup Services, India\n",
            "    duration: Jan 2021 - Nov 2021\n",
            "    achievements:\n",
            "      - Analyzed supplier costs and payment credit periods using data-driven insights, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.\n",
            "      - Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and a 20% improvement in operational efficiency.\n",
            "      - Built Python-based automation scripts to optimize internal workflows, reducing manual effort by 25% and enhancing team efficiency.\n",
            "  - job_title: Procurement Engineer\n",
            "    company: Valeo India, India\n",
            "    duration: Jun 2018 - Jan 2021\n",
            "    achievements:\n",
            "      - Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.\n",
            "      - Developed Power BI dashboards to track procurement KPIs, cutting reporting time by 40% and enhancing supplier performance monitoring.\n",
            "      - Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.\n",
            "\n",
            "projects:\n",
            "  - project_name: Meta Financial Dashboard\n",
            "    keywords: [Tableau, financial KPIs, data analysis]\n",
            "    description: Developed an interactive Tableau dashboard to analyze key financial KPIs, delivering actionable insights and improving financial decision-making.\n",
            "  - project_name: Customer Churn Analysis\n",
            "    keywords: [Python, ETL pipeline, predictive analytics]\n",
            "    description: Designed and implemented an ETL pipeline in Python to clean and process large ecommerce customer datasets, performing predictive analytics to identify high-risk churn customers.\n",
            "  - project_name: Vestas Process Automation\n",
            "    keywords: [Power BI, Python, Excel, automation]\n",
            "    description: Automated employee data collection, analysis, and visualization processes using Power BI, Python, and Excel, improving traceability and key metric visibility.\n",
            "  - project_name: SAP Data Automation\n",
            "    keywords: [SAP, Excel, automation, data management]\n",
            "    description: Developed automation solutions to streamline data management between Excel and SAP systems, enabling seamless updates of Excel data into SAP and enhancing operational efficiency.\n",
            "  - project_name: 3D Data Visualization Project\n",
            "    keywords: [Python, Matplotlib, 3D data visualization]\n",
            "    description: Developed a 3D data visualization tool using Python and Matplotlib to represent and visualize complex data, generating new customer deliverables and improving data analysis capabilities.\n",
            "  - project_name: Image Processing Project\n",
            "    keywords: [Python, OpenCV, image processing, object detection]\n",
            "    description: Developed an image processing tool using Python and OpenCV to analyze and visualize image data, applying techniques such as object detection and image segmentation to improve data analysis capabilities.\n",
            "\n",
            "education:\n",
            "  - institution: St. Clair College\n",
            "    degree: Diploma in Data Analytics for Business\n",
            "    completion_date: 2024\n",
            "  - institution: St. Joseph’s Institute of Technology\n",
            "    degree: Bachelor of Engineering, Mechanical\n",
            "    completion_date: 2024\n",
            "\n",
            "certifications:\n",
            "  - certification: SQL for Data Science\n",
            "    issuer: Coursera\n",
            "  - certification: Python for Data Analysis\n",
            "    issuer: Udemy\n",
            "  - certification: Tableau Desktop Specialist Certification\n",
            "    issuer: Tableau\n"
          ]
        }
      ],
      "source": [
        "print(llm_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bNknMr22vdK"
      },
      "source": [
        "Reading yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyyaml\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
            "Installing collected packages: pyyaml\n",
            "Successfully installed pyyaml-6.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_2t7cYa253e8"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "new_data = yaml.safe_load(llm_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJOasVQv54fS",
        "outputId": "a4cfad9b-8ba1-447e-c949-0f2a749d7ed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'work_experience': [{'title': 'Procurement Engineer',\n",
              "   'company': 'Vestas Wind Technology, India',\n",
              "   'dates': 'Nov 2021 - Aug 2023',\n",
              "   'achievements': ['Developed and maintained Python-based automation tools to streamline procurement workflows, resulting in a 15% reduction in procurement costs and a 20% improvement in project timelines.',\n",
              "    'Designed and deployed Power BI dashboards to track KPIs, supplier performance, and category spend, enabling data-driven decision-making and a 25% reduction in manual efforts.',\n",
              "    'Utilized predictive analytics and data visualization to optimize material master data accuracy, reducing errors by 25% and improving data consistency.']},\n",
              "  {'title': 'Consultant ECM Purchaser',\n",
              "   'company': 'Vestas, ManpowerGroup Services, India',\n",
              "   'dates': 'Jan 2021 - Nov 2021',\n",
              "   'achievements': ['Analyzed supplier costs and payment credit periods using data-driven insights, identifying potential savings of €3M and implementing standardized and efficient procurement strategies.',\n",
              "    'Developed and deployed cloud-based automation tools, resulting in a 30% increase in team productivity and a 20% improvement in operational efficiency.',\n",
              "    'Built Python-based automation scripts to optimize internal workflows, reducing manual effort by 25% and enhancing team efficiency.']},\n",
              "  {'title': 'Procurement Engineer',\n",
              "   'company': 'Valeo India, India',\n",
              "   'dates': 'Jun 2018 - Jan 2021',\n",
              "   'achievements': ['Managed RFx processes across global wind turbine segments, achieving 10-15% cost reductions through competitive vendor sourcing and procurement strategies.',\n",
              "    'Developed Power BI dashboards to track procurement KPIs, cutting reporting time by 40% and enhancing supplier performance monitoring.',\n",
              "    'Utilized Python and SQL to analyze procurement data, uncovering opportunities for cost optimization worth $500K+ annually.']}],\n",
              " 'projects': [{'title': 'Meta Financial Dashboard',\n",
              "   'description': 'Developed an interactive Tableau dashboard to analyze key financial KPIs, delivering actionable insights and improving financial decision-making.'},\n",
              "  {'title': 'Customer Churn Analysis',\n",
              "   'description': 'Designed and implemented an ETL pipeline in Python to clean and process large ecommerce customer datasets, performing predictive analytics to identify high-risk churn customers.'},\n",
              "  {'title': 'Vestas Process Automation',\n",
              "   'description': 'Automated employee data collection, analysis, and visualization processes using Power BI, Python, and Excel, improving traceability and key metric visibility.'},\n",
              "  {'title': 'SAP Data Automation',\n",
              "   'description': 'Developed automation solutions to streamline data management between Excel and SAP systems, enabling seamless updates of Excel data into SAP and enhancing operational efficiency.'},\n",
              "  {'title': '3D Data Visualization Project',\n",
              "   'description': 'Developed a 3D data visualization tool using Python and Matplotlib to represent and visualize complex data, generating new customer deliverables and improving data analysis capabilities.'},\n",
              "  {'title': 'Image Processing Project',\n",
              "   'description': 'Developed an image processing tool using Python and OpenCV to analyze and visualize image data, applying techniques such as object detection and image segmentation to improve data analysis capabilities.'}],\n",
              " 'education': [{'diploma': 'Diploma in Data Analytics for Business',\n",
              "   'institution': 'St. Clair College',\n",
              "   'date': 2024},\n",
              "  {'bachelor': 'Bachelor of Engineering, Mechanical',\n",
              "   'institution': 'St. Joseph’s Institute of Technology',\n",
              "   'date': 2024}],\n",
              " 'certifications': [{'name': 'SQL for Data Science', 'issuer': 'Coursera'},\n",
              "  {'name': 'Python for Data Analysis', 'issuer': 'Udemy'},\n",
              "  {'name': 'Tableau Desktop Specialist Certification', 'issuer': 'Tableau'}]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Developed and maintained Python-based automation tools to streamline procurement workflows, resulting in a 15% reduction in procurement costs and a 20% improvement in project timelines.',\n",
              " 'Designed and deployed Power BI dashboards to track KPIs, supplier performance, and category spend, enabling data-driven decision-making and a 25% reduction in manual efforts.',\n",
              " 'Utilized predictive analytics and data visualization to optimize material master data accuracy, reducing errors by 25% and improving data consistency.']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_data['work_experience'][0]['achievements']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71arXCTp3Mrd",
        "outputId": "48dad52b-c973-47d6-9ef4-51436cbf2f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YAML file updated successfully.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "\n",
        "# Load your existing YAML file\n",
        "yaml_file_path = r'C:\\Users\\karth\\OneDrive\\Documents\\GitHub\\pdf-build\\test2.yaml'\n",
        "with open(yaml_file_path, 'r') as f:\n",
        "    resume_data = yaml.safe_load(f)\n",
        "\n",
        "# Load the new YAML content from the LLM output\n",
        "new_data = yaml.safe_load(llm_yaml)\n",
        "\n",
        "# # Update specific parts of your resume, e.g., work and projects sections\n",
        "for i in range(3):\n",
        "    resume_data[\"work\"][i]['highlights'] = new_data['work_experience'][i]['achievements']\n",
        "    resume_data['projects'][i]['name'] = new_data['projects'][i]['project_name']\n",
        "    resume_data['projects'][i]['description'] = new_data['projects'][i]['description']\n",
        "    resume_data['projects'][i]['keywords'] = new_data['projects'][i]['keywords']\n",
        "\n",
        "# # Write the updated YAML back to the file\n",
        "with open(yaml_file_path, 'w') as f:\n",
        "    yaml.safe_dump(resume_data, f, default_flow_style=False)\n",
        "\n",
        "print(\"YAML file updated successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1QNSUj5h1a",
        "outputId": "0d3c89d2-381c-48ba-b7e7-2ff54b43f34b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q weasyprint\n",
        "!apt-get -qq install -y libpango-1.0-0 libpangocairo-1.0-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement libpango-1.0-0 (from versions: none)\n",
            "ERROR: No matching distribution found for libpango-1.0-0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q jinja2 jsonschema libpango-1.0-0 libpangocairo-1.0-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sudo is disabled on this machine. To enable it, go to the \u001b]8;;ms-settings:developers\u001b\\Developer Settings page\u001b]8;;\u001b\\ in the Settings app\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get -qq install -y libpango-1.0-0 libpangocairo-1.0-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated HTML content:\n",
            "Attempting to create resume PDF at: test2.pdf\n",
            "the directory is \n"
          ]
        }
      ],
      "source": [
        "!python resumy.py build -o test2.pdf test2.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated HTML content:\n",
            "Attempting to create resume PDF at: resume.pdf\n",
            "the directory is \n"
          ]
        }
      ],
      "source": [
        "!python resumy.py build -o resume.pdf karthik.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMBXeSyf5Vag",
        "outputId": "6a60f3c8-c6f7-41b0-e35b-1702667c4626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated HTML content:\n",
            "Attempting to create resume PDF at: test2.pdf\n",
            "the directory is \n",
            "SUCCESS: PDF created at test2.pdf\n"
          ]
        }
      ],
      "source": [
        "!python /content/pdf-build/resumy.py build -o test2.pdf /content/pdf-build/test2.yaml"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPCaZx/Db0SJumCv0pb9W3i",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/KarthikeyanBaskaran/pdf-build/blob/main/Resume_Generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
